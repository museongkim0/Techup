{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baecc744-d021-4596-a96a-9e04d2066017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rating_df = pd.read_csv(\"../data/test_rating.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd92b0b-e4e7-4b89-b9e5-d4cd6cb4366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 사용자 수: 28921\n",
      "슬라이스된 사용자 수: 10000\n",
      "슬라이스된 데이터프레임 크기: (34284, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_idx</th>\n",
       "      <th>user_idx</th>\n",
       "      <th>product_idx</th>\n",
       "      <th>review_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>861</td>\n",
       "      <td>1690</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5391</td>\n",
       "      <td>3953</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>6266</td>\n",
       "      <td>1345</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>4427</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>8323</td>\n",
       "      <td>769</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_idx  user_idx  product_idx  review_rating\n",
       "2            3       861         1690              4\n",
       "3            4      5391         3953              5\n",
       "9           10      6266         1345              4\n",
       "12          13      4427           61              2\n",
       "19          20      8323          769              5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 고유한 user_idx 목록 추출\n",
    "unique_users = set(rating_df['user_idx'])\n",
    "\n",
    "# 상위 2만 개 사용자 선택\n",
    "selected_users = list(unique_users)[:10000]\n",
    "\n",
    "# 선택된 사용자에 해당하는 데이터프레임 슬라이스\n",
    "sliced_df = rating_df[rating_df['user_idx'].isin(selected_users)]\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"원래 사용자 수: {len(unique_users)}\")\n",
    "print(f\"슬라이스된 사용자 수: {len(set(sliced_df['user_idx']))}\")\n",
    "print(f\"슬라이스된 데이터프레임 크기: {sliced_df.shape}\")\n",
    "sliced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ad0752-18ba-4688-8561-51f03bff6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = sliced_df.pivot_table(\n",
    "        index='user_idx',\n",
    "        columns='product_idx',\n",
    "        values='review_rating',\n",
    "        fill_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb0ad7e-f7ab-4dc6-b19b-f5af417c27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType, StructType, StructField\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"elasticsearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851381e2-9705-4056-b3ba-00940b9da1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"spark://spark-master:7077\") \\\n",
    "#     .appName(\"JupyterSparkApp\") \\\n",
    "#     .config(\"spark.jars\", \"/usr/local/spark/jars/elasticsearch-spark-30_2.12-8.11.0.jar\") \\\n",
    "#     .getOrCreate()\n",
    "# 새 SparkSession 초기화\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"Write to Elasticsearch\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/elasticsearch-spark-30_2.12-8.11.0.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/usr/local/spark/jars/elasticsearch-spark-30_2.12-8.11.0.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/usr/local/spark/jars/elasticsearch-spark-30_2.12-8.11.0.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f16dc2-cf89-42d6-b0dd-ce6c197f1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idx_list = user_item_matrix.index\n",
    "item_idx_list = user_item_matrix.columns\n",
    "\n",
    "user_matrix_sparse = csr_matrix(user_item_matrix.values)\n",
    "item_matrix_sparse = csr_matrix(user_item_matrix.T.values)\n",
    "\n",
    "n_components = 128  # 차원 수 (하이퍼파라미터, 튜닝 가능)\n",
    "user_svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "item_svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "User_svd = user_svd.fit_transform(user_matrix_sparse)\n",
    "Item_svd = item_svd.fit_transform(item_matrix_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b7107ec-7464-4a34-8bc4-fe797805e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_spark_df(spark, pandas_df, entity_type):\n",
    "    \"\"\"Pandas DataFrame을 Spark DataFrame으로 변환\"\"\"\n",
    "    # 벡터 컬럼을 문자열로 변환 (JSON 형식)\n",
    "    pandas_df['vector_json'] = pandas_df['vector'].apply(lambda x: json.dumps(x.tolist()))\n",
    "\n",
    "    # Spark DataFrame 스키마 정의\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"vector_json\", StringType(), False),\n",
    "        StructField(\"last_updated\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    # Spark DataFrame 생성\n",
    "    spark_df = spark.createDataFrame(\n",
    "        pandas_df[['id', 'vector_json', 'last_updated']],\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    # 벡터 문자열을 배열로 변환하는 UDF 정의\n",
    "    vector_to_array_udf = udf(lambda x: json.loads(x), ArrayType(FloatType()))\n",
    "\n",
    "    # UDF 적용하여 벡터 변환\n",
    "    result_df = spark_df.withColumn(\"vector\", vector_to_array_udf(col(\"vector_json\"))) \\\n",
    "        .drop(\"vector_json\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def es_write_with_spark(df, index_name):\n",
    "    \"\"\"Spark DataFrame을 Elasticsearch에 저장\"\"\"\n",
    "    print(f\"Writing records to Elasticsearch index '{index_name}'\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Elasticsearch에 데이터 쓰기 (Spark Elasticsearch connector 사용)\n",
    "    df.write \\\n",
    "        .format(\"org.elasticsearch.spark.sql\") \\\n",
    "        .option(\"es.nodes\", \"elasticsearch\") \\\n",
    "        .option(\"es.port\", \"9200\") \\\n",
    "        .option(\"es.net.http.header.Accept\", \"application/vnd.elasticsearch+json; compatible-with=8\") \\\n",
    "        .option(\"es.net.http.header.Content-Type\", \"application/json\") \\\n",
    "        .option(\"es.mapping.id\", \"id\") \\\n",
    "        .option(\"es.write.operation\", \"upsert\") \\\n",
    "        .option(\"es.batch.size.entries\", \"1000\") \\\n",
    "        .option(\"es.batch.write.retry.count\", \"3\") \\\n",
    "        .option(\"es.batch.write.retry.wait\", \"10s\") \\\n",
    "        .option(\"es.nodes.wan.only\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(index_name)\n",
    "\n",
    "    print(f\"Finished writing to Elasticsearch index '{index_name}'\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    logger.info(f\"write_with_spark in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "def create_vector_indices(es, n_components):\n",
    "    \"\"\"Elasticsearch 벡터 인덱스 생성\"\"\"\n",
    "    # 벡터 인덱스 매핑 설정\n",
    "    vector_mapping = {\n",
    "        \"properties\": {\n",
    "            \"vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": n_components,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\",\n",
    "                \"index_options\": {\n",
    "                    \"type\": \"hnsw\",  # HNSW 알고리즘 사용\n",
    "                    \"m\": 16,  # 최대 연결 수 (기본값)\n",
    "                    \"ef_construction\": 200  # 인덱스 생성 시 효율성 파라미터\n",
    "                }\n",
    "            },\n",
    "            \"last_updated\": {\n",
    "                \"type\": \"date\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 인덱스 설정\n",
    "    vector_settings = {\n",
    "        \"index\": {\n",
    "            \"number_of_shards\": 3,\n",
    "            \"number_of_replicas\": 1\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 인덱스 목록\n",
    "    indices = [\"user-based\", \"item-based\"]\n",
    "\n",
    "    for index_name in indices:\n",
    "        # 인덱스가 존재하면 삭제 후 다시 생성\n",
    "        if es.indices.exists(index=index_name):\n",
    "            print(f\"Deleting existing index: {index_name}\")\n",
    "            es.indices.delete(index=index_name)\n",
    "\n",
    "        # 인덱스 생성\n",
    "        print(f\"Creating index: {index_name}\")\n",
    "        es.indices.create(\n",
    "            index=index_name,\n",
    "            mappings=vector_mapping,\n",
    "            settings=vector_settings\n",
    "        )\n",
    "        print(f\"Finished Creating index: {index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c59ad79-d2eb-436c-a3ae-424452ece158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엘라스틱서치 연결\n",
    "es = Elasticsearch(\n",
    "    [\"http://elasticsearch:9200\"],\n",
    "    headers={\n",
    "        \"Accept\": \"application/vnd.elasticsearch+json; compatible-with=8\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941a9a08-adf2-4942-9813-dea6191d0b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:HEAD http://elasticsearch:9200/user-based [status:200 duration:0.342s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting existing index: user-based\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:DELETE http://elasticsearch:9200/user-based [status:200 duration:0.920s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index: user-based\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://elasticsearch:9200/user-based [status:200 duration:1.492s]\n",
      "INFO:elastic_transport.transport:HEAD http://elasticsearch:9200/item-based [status:200 duration:0.079s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Creating index: user-based\n",
      "Deleting existing index: item-based\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:DELETE http://elasticsearch:9200/item-based [status:200 duration:0.228s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index: item-based\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://elasticsearch:9200/item-based [status:200 duration:0.968s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Creating index: item-based\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 생성\n",
    "create_vector_indices(es, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aaaaaa7-e931-442b-813c-cfacb117287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing records to Elasticsearch index 'user-based'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elasticsearch:write_with_spark in 54.21 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing to Elasticsearch index 'user-based'\n",
      "Writing records to Elasticsearch index 'item-based'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elasticsearch:write_with_spark in 10.63 seconds\n",
      "INFO:elasticsearch:Spark finished time in 70.93 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing to Elasticsearch index 'item-based'\n",
      "Vector update completed with PySpark\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 현재 시간\n",
    "current_time = datetime.now().isoformat()\n",
    "\n",
    "# Pandas DataFrame 생성 (User 벡터)\n",
    "user_df = pd.DataFrame({\n",
    "    'id': user_idx_list,\n",
    "    'vector': list(User_svd),\n",
    "    'last_updated': current_time\n",
    "})\n",
    "\n",
    "# Pandas DataFrame 생성 (Item 벡터)\n",
    "item_df = pd.DataFrame({\n",
    "    'id': item_idx_list,\n",
    "    'vector': list(Item_svd),\n",
    "    'last_updated': current_time\n",
    "})\n",
    "\n",
    "# Spark DataFrame으로 변환\n",
    "user_spark_df = convert_to_spark_df(spark, user_df, \"user\")\n",
    "item_spark_df = convert_to_spark_df(spark, item_df, \"item\")\n",
    "\n",
    "# Elasticsearch에 저장\n",
    "es_write_with_spark(user_spark_df, \"user-based\")\n",
    "es_write_with_spark(item_spark_df, \"item-based\")\n",
    "\n",
    "end_time = time.time()\n",
    "logger.info(f\"Spark finished time in {end_time - start_time:.2f} seconds\")\n",
    "print(\"Vector update completed with PySpark\")\n",
    "\n",
    "# Spark 세션 종료\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53103929-5282-4586-8eb8-f707dfdcf0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing records to Elasticsearch index 'user-based'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elasticsearch:write_with_spark in 56.12 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing to Elasticsearch index 'user-based'\n",
      "Writing records to Elasticsearch index 'item-based'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elasticsearch:write_with_spark in 14.20 seconds\n",
      "INFO:elasticsearch:Spark finished time in 84.98 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing to Elasticsearch index 'item-based'\n",
      "Vector update completed with PySpark\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "498cf7a3-388b-42bc-a01a-0cf9cf0cd301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:GET http://elasticsearch:9200/_cat/indices?format=json [status:200 duration:0.167s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 인덱스 리스트 ===\n",
      "인덱스: user-based, 문서 수: 10000\n",
      "인덱스: employees, 문서 수: 1\n",
      "인덱스: item-based, 문서 수: 4186\n"
     ]
    }
   ],
   "source": [
    "# 1. 인덱스 리스트 조회\n",
    "print(\"=== 인덱스 리스트 ===\")\n",
    "indices = es.cat.indices(format=\"json\")\n",
    "for index in indices:\n",
    "    index_name = index['index']\n",
    "    doc_count = index['docs.count']\n",
    "    print(f\"인덱스: {index_name}, 문서 수: {doc_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be26ecdc-3d33-4827-ab9f-dc842c3d0d39",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o70.showString.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2390)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43muser_spark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o70.showString.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2390)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "user_spark_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e042d93-786d-491e-b6a8-ed90e5360110",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369bfae4-2c1c-4cf4-b215-298b941a25b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
